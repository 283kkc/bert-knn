{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 山田さんが 顔 を見たのはこれが初めてでした 。 巨大だった 。 [SEP]\n",
      "[CLS] 山田さんが 面倒 を見たのはこれが初めてでした 。 巨大だった 。 [SEP]\n",
      "[CLS] 山田さんが 目 を見たのはこれが初めてでした 。 巨大だった 。 [SEP]\n",
      "[CLS] 山田さんが 夢 を見たのはこれが初めてでした 。 巨大だった 。 [SEP]\n",
      "[CLS] 山田さんが 姿 を見たのはこれが初めてでした 。 巨大だった 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.modeling_bert import BertForMaskedLM\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"bert/ja/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert/ja/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask\")\n",
    "model.eval()\n",
    "\n",
    "input_ids = tokenizer.encode(f\"\"\"\n",
    "山田さんが{tokenizer.mask_token}を見たのはこれが初めてでした。巨大だった。\n",
    "\"\"\", return_tensors=\"pt\")\n",
    "\n",
    "masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "\n",
    "result = model(input_ids)\n",
    "result = result[0][:, masked_index].topk(5).indices.tolist()[0]\n",
    "\n",
    "for r in result:\n",
    "    output = input_ids[0].tolist()\n",
    "    output[masked_index] = r\n",
    "    print(tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 240])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\" 日曜日の16時頃に訪問。 お店の前は通った事がありましたが 今回やっと初訪問。\n",
    "\n",
    "この日はセールをやっていたようで\n",
    "いつも外から眺めている印象よりお客さんが多いですね。\n",
    "\n",
    "様々な種類のお肉が売っておりましたが\n",
    "既に加工していたり揚げていたりする商品もあったので\n",
    "そちらを中心に物色します。\n",
    "\n",
    "コロッケやメンチカツなどもありましたが\n",
    "唐揚げと串揚げを購入。\n",
    "\n",
    "唐揚げはグラム数で計り売りで2個で95円でした。\n",
    "串揚げは150円でした。\n",
    "\n",
    "その日の夜に特に温めずに頂きましたが\n",
    "唐揚げは温めなくてもジューシーな味わいで\n",
    "個人的には非常に好みの味。\n",
    "\n",
    "串揚げは中に玉葱と挽肉が入っておりましたが\n",
    "中身はほとんど玉葱のみで、こちらは正直口には合わなかったですね。\n",
    "\n",
    "それでも唐揚げはさすがお肉屋さんの味といった感想でした。\n",
    "\n",
    "ご馳走様でした。\n",
    "\"\"\"\n",
    "tokenizer.encode(text, return_tensors='pt').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"bert/ja/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert/ja/BERT-base_mecab-ipadic-bpe-32k_whole-word-mask\")\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 240, 32000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text = \"こんにちは\"\n",
    "input_ids = torch.tensor([tokenizer.encode(short_text, add_special_tokens=False)])\n",
    "with torch.no_grad():\n",
    "#     last_hidden_states = model(input_ids)[0]\n",
    "#     second_layer = model(input_ids)[-2]\n",
    "    all_layer = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_layer[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.6648,  3.5317, -5.0330,  ..., -5.0268, -8.2822, -3.7546],\n",
       "        [-6.6670,  4.8843, -1.5880,  ..., -7.1451, -5.1611, -5.6547],\n",
       "        [-6.7644,  5.2466, -0.7674,  ..., -6.9232, -5.4712, -5.9564]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_layer[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MeCab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-39ba59a50d5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertJapaneseTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-japanese-whole-word-masking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-japanese-whole-word-masking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/bert_client_api/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/bert_client_api/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# Instantiate tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             OSError(\"Unable to load vocabulary from file. \"\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/bert_client_api/lib/python3.6/site-packages/transformers/tokenization_bert_japanese.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_word_tokenize, do_subword_tokenize, word_tokenizer_type, subword_tokenizer_type, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mword_tokenizer_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mecab'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 self.word_tokenizer = MecabTokenizer(do_lower_case=do_lower_case,\n\u001b[0;32m--> 125\u001b[0;31m                                                      never_split=never_split)\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/bert_client_api/lib/python3.6/site-packages/transformers/tokenization_bert_japanese.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, do_lower_case, never_split, normalize_text)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmecab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MeCab'"
     ]
    }
   ],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"bert-base-japanese-whole-word-masking\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['お', '##腹', 'が', '痛', '##い', 'ので', '遅れ', 'ます', '。']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MeCabをimportしないとno module named mecabでエラー\n",
    "# no vocab text はタイプミスの可能性。\n",
    "# キャッシュされる。\n",
    "# https://qiita.com/nekoumei/items/7b911c61324f16c43e7e\n",
    "import MeCab\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "tokenizer.tokenize('お腹が痛いので遅れます。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "すもも\tスモモ\tすもも\t名詞-一般\t\t\n",
      "も\tモ\tも\t助詞-係助詞\t\t\n",
      "もも\tモモ\tもも\t名詞-一般\t\t\n",
      "も\tモ\tも\t助詞-係助詞\t\t\n",
      "もも\tモモ\tもも\t名詞-一般\t\t\n",
      "の\tノ\tの\t助詞-連体化\t\t\n",
      "うち\tウチ\tうち\t名詞-非自立-副詞可能\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger(\"-Ochasen\")\n",
    "\n",
    "print(mecab.parse(\"すもももももももものうち\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[   2, 4609,    3]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "import MeCab\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "text = \"テスト\"\n",
    "input_ids = torch.tensor([tokenizer.encode(text)]).unsqueeze(0)\n",
    "print(input_ids)\n",
    "model = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "print('loaded')\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    last_hiddden_states = outputs[0]\n",
    "last_hiddden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
